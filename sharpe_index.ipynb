{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHsYcXLbR5vN"
      },
      "source": [
        "# Sharpe Ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old"
      ],
      "metadata": {
        "id": "Rx4t6KfsVShS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ctG0LXQWR5vO",
        "outputId": "d01cac74-1995-44a3-b8a2-d7d86c28da30"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-502062ff195b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m  \u001b[0;31m# Yahoo Finance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtsaplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarimax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSARIMAX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcalendar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonthrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/graphics/tsaplots.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/stattools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_next_regular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/compat/scipy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_scipy_multivariate_t\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultivariate_t\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultivariate_t\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_continuous_distns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_levy_stable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlevy_stable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_discrete_distns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf  # Yahoo Finance\n",
        "\n",
        "from statsmodels.graphics import tsaplots\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from calendar import monthrange\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.dates as mdates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLmcgs3qR5vP"
      },
      "source": [
        "In this notebook, we will be attempting to predict the sharpe ratio of several commodity futures. All data in this notebook was collected via Yahoo Finance's API. We will predict the sharpe ratio using the futures with the nearest expiration dates, specifically:\n",
        "\n",
        "1. Corn Futures (ZC=F), daily close values.\n",
        "2. Soybean Futures (ZS=F), daily close values.\n",
        "3. Wheat Futures (ZW=F), daily close values.\n",
        "4. Oats Futures (ZO=F), daily close values.\n",
        "5. Rice Futures (ZR=F), daily close values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBHoyf3sR5vQ"
      },
      "source": [
        "We start by retrieving information of the risk-free stock, which in this case we chose to be ^IRX (13 WEEK USA TREASURY BILL). All values used will be CLOSE values. The data collected is of the last 10 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XCd6kC0R5vQ"
      },
      "outputs": [],
      "source": [
        "rf = yf.Ticker('^IRX')\n",
        "rf = rf.history(period='max')\n",
        "rf['Datetime'] = rf.index\n",
        "rf.reset_index(drop=True, inplace=True)\n",
        "rf['Date'] = rf['Datetime'].dt.date\n",
        "rf['Return'] = rf['Close'].pct_change()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIV7ZElPR5vQ"
      },
      "outputs": [],
      "source": [
        "s = yf.Ticker('SB=F')\n",
        "s.info['shortName']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_aHwzFTR5vR"
      },
      "source": [
        "Next, we retrieve data from the last 10 years of all commodity prices, those listed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pof-i5ehR5vR"
      },
      "outputs": [],
      "source": [
        "products = {'Corn': 'ZC',\n",
        "            'Soybean': 'ZS',\n",
        "            'Wheat': 'ZW',\n",
        "            'Oats': 'ZO',\n",
        "            'Rice': 'ZR',}\n",
        "\n",
        "df_futures = pd.DataFrame()\n",
        "\n",
        "for product in products:\n",
        "    value = f'{products[product]}=F'\n",
        "    futures = yf.Ticker(value)\n",
        "    futures = futures.history(period='max')\n",
        "    df_futures[product] = futures['Close']\n",
        "\n",
        "\n",
        "df_futures['Datetime'] = df_futures.index\n",
        "df_futures.reset_index(drop=True, inplace=True)\n",
        "df_futures['Date'] = df_futures['Datetime'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuSMHHyVR5vR"
      },
      "outputs": [],
      "source": [
        "df_futures.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_EZpQrpR5vS"
      },
      "outputs": [],
      "source": [
        "# get the dataframe values on the date 2018-01-23\n",
        "df_futures[df_futures['Date'] == pd.to_datetime('2018-01-24').date()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPU1VSfKR5vS"
      },
      "outputs": [],
      "source": [
        "df_futures = df_futures.merge(rf[['Date', 'Return']], on='Date', how='left')\n",
        "df_futures.rename(columns={'Return': 'rf'}, inplace=True)\n",
        "df_futures.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAwByi-PR5vS"
      },
      "outputs": [],
      "source": [
        "# compute sharp ratio for each product in each year\n",
        "sharp_ratios = {}\n",
        "\n",
        "for product in products:\n",
        "    df_futures[f'{product}_return'] = df_futures[product].pct_change()\n",
        "    df_futures[f'{product}_excess_return'] = df_futures[f'{product}_return'] - df_futures['rf']\n",
        "    # compute sharp ratio for each year\n",
        "    sharp_ratios[product] = {}\n",
        "    for year in range(2015, 2024):  # only data with full years\n",
        "        df_year = df_futures[df_futures['Datetime'].dt.year == year]\n",
        "        sharp_ratio = df_year[f'{product}_excess_return'].mean() / df_year[f'{product}_excess_return'].std()\n",
        "        annualized_sharp_ratio = sharp_ratio * (252 ** 0.5)\n",
        "        sharp_ratios[product][year] = annualized_sharp_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs-9LjHSR5vS"
      },
      "outputs": [],
      "source": [
        "print('Yearly Sharpe Ratio For Each Product (Compared to Risk Free Rate based on 3 month treasury bill)')\n",
        "for product in sharp_ratios:\n",
        "    print(product)\n",
        "    for year in sharp_ratios[product]:\n",
        "        print(f'{year}: {sharp_ratios[product][year]}')\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcvg1edR5vT"
      },
      "source": [
        "Now, we will compute the sharpe ratio on a bi-weekly basis, making it so that we will have more sharpe ratios that we can train a model on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn97k5fmR5vT"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 14\n",
        "\n",
        "normalized_sharpe_ratios = {}\n",
        "\n",
        "for product in products:\n",
        "    df_futures[f'{product}_return'] = df_futures[product].pct_change()\n",
        "    df_futures[f'{product}_excess_return'] = df_futures[f'{product}_return'] - df_futures['rf']\n",
        "    rolling_mean = df_futures[f'{product}_excess_return'].rolling(window=WINDOW_SIZE, step=WINDOW_SIZE).mean()\n",
        "    rolling_std = df_futures[f'{product}_excess_return'].rolling(window=WINDOW_SIZE, step=WINDOW_SIZE).std()\n",
        "    normalized_sharpe_ratios[product] = rolling_mean / rolling_std * (WINDOW_SIZE ** 0.5)\n",
        "    # add column of date\n",
        "    normalized_sharpe_ratios[product]['Datetime'] = df_futures['Datetime']\n",
        "\n",
        "# plot normalized sharp ratio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "\n",
        "normalized_sharpe_ratios['Corn']\n",
        "\n",
        "# # plot normalized sharp ratio in 4 subplots\n",
        "# fig, axs = plt.subplots(5, 1, figsize=(10, 10))\n",
        "# fig.suptitle('Normalized Sharpe Ratios for Corn, Soybean, Wheat, and Oats')\n",
        "# for i, product in enumerate(products):\n",
        "#     axs[i].plot(df_futures['Datetime'], normalized_sharpe_ratios[product], label=product)\n",
        "#     axs[i].xaxis.set_major_locator(mdates.YearLocator())\n",
        "#     axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "#     axs[i].set_title(product)\n",
        "#     axs[i].legend()\n",
        "#     axs[i].grid()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# (normalized_sharpe_ratios['Corn'].dropna() - normalized_sharpe_ratios['Soybean'].dropna()).mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LSTMs to Predict Sharpe Ratio of the Next Quarter"
      ],
      "metadata": {
        "id": "_aPKj-iyVM2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf  # Yahoo Finance"
      ],
      "metadata": {
        "id": "XCSCZ2bQwADG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "VlDxOTJGVI2H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqmVQj8tR5vT",
        "outputId": "f2ddf7f9-c2be-4493-f701-b5ec0ecf21be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load word vectors\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(string):\n",
        "    sol = np.zeros(100)\n",
        "    for word in string.lower().split():\n",
        "        try:\n",
        "            sol += word_vectors[word]\n",
        "        except:\n",
        "            continue\n",
        "    return sol"
      ],
      "metadata": {
        "id": "WsqgXPUXo91s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('commodities_tickers.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "categories = data.keys()\n",
        "categories_vectors = {category: get_vector(category) for category in categories}\n",
        "\n",
        "data_reshaped = {key2: [key1, value2] for key1, value1 in data.items() for key2, value2 in value1.items()}\n",
        "\n",
        "# make matrix of word vectors for each commodity\n",
        "commodities = data_reshaped.keys()\n",
        "commodities_vectors = np.array([get_vector(commodity) for commodity in commodities])\n",
        "\n",
        "# apply PCA to reduce the dimensionality of the word vectors\n",
        "pca = PCA(n_components=5)\n",
        "commodities_vectors_pca = pca.fit_transform(commodities_vectors)\n",
        "categories_vectors_pca = pca.transform(np.array(list(categories_vectors.values())))\n",
        "categories_vectors_pca = {key: value for key, value in zip(categories, categories_vectors_pca)}\n",
        "\n",
        "data_reshaped = {key: [np.concatenate([categories_vectors_pca[value[0]], commodities_vectors_pca[i]]), value[1]] for i, (key, value) in enumerate(data_reshaped.items())}"
      ],
      "metadata": {
        "id": "pPkS4Ud9m0no"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RISK_FREE = yf.Ticker('^IRX')\n",
        "RISK_FREE = RISK_FREE.history(period='max')\n",
        "RISK_FREE['Datetime'] = RISK_FREE.index\n",
        "RISK_FREE.reset_index(drop=True, inplace=True)\n",
        "RISK_FREE['Date'] = RISK_FREE['Datetime'].dt.date\n",
        "RISK_FREE['Returns-RF'] = RISK_FREE['Close'].pct_change()\n",
        "\n",
        "def retrieve_feature(product, data_reshaped):\n",
        "    return yf.Ticker(data_reshaped[product][1]).history(period='max')\n",
        "\n",
        "def feature_extraction(data):\n",
        "    data['Datetime'] = data.index\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data['Date'] = data['Datetime'].dt.date\n",
        "    data.dropna(inplace=True)\n",
        "    data['Return'] = data['Close'].pct_change()\n",
        "    return data[['Datetime', 'Date', 'Return']]\n",
        "\n",
        "def risk_free_return(data):\n",
        "    data = data.merge(RISK_FREE[['Date', 'Returns-RF']], on='Date', how='left')\n",
        "    data.rename(columns={'Returns-RF': 'rf'}, inplace=True)\n",
        "    data.dropna(inplace=True)\n",
        "    data['Risk-Adjusted Return'] = data['Return'] - data['rf']\n",
        "    return data[['Datetime', 'Risk-Adjusted Return']]\n",
        "\n",
        "def sharpe_ratio(data):\n",
        "    # add feature of quarter such that jan/feb/mar -> 1, apr/may/jun -> 2, jul/aug/sep -> 3, oct/nov/dec -> 4\n",
        "    data['Quarter'] = data['Datetime'].dt.quarter\n",
        "    # group by quarter and year, compute the mean and standard deviation of risk-adjusted return\n",
        "    data_grouped = data.groupby([data['Datetime'].dt.year, 'Quarter'])\n",
        "\n",
        "    mean = data_grouped['Risk-Adjusted Return'].mean()\n",
        "    std = data_grouped['Risk-Adjusted Return'].std()\n",
        "    length = data_grouped['Risk-Adjusted Return'].count()\n",
        "\n",
        "    sharp_ratio = mean / std * (length ** 0.5)\n",
        "    return sharp_ratio.reset_index()"
      ],
      "metadata": {
        "id": "c3UJcD9OP7cy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_series(product, data_reshaped):\n",
        "    data = retrieve_feature(product, data_reshaped)\n",
        "    data = feature_extraction(data)\n",
        "    data = risk_free_return(data)\n",
        "    sharpe = sharpe_ratio(data)\n",
        "    values = sharpe.values\n",
        "    # for every row in values, concatenate the corresponding vector in data_reshaped\n",
        "    meta_data = data_reshaped[product][0]\n",
        "    # copy meta_data for each row in values\n",
        "    meta_data = np.tile(meta_data, (values.shape[0], 1))\n",
        "    return np.concatenate([meta_data, values], axis=1)\n"
      ],
      "metadata": {
        "id": "gRQAYSZDP9ND"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = [torch.tensor(get_final_series(product, data_reshaped)) for product in data_reshaped.keys() if product not in ['Corn', 'Soybeans', 'Wheat', 'Oats']]\n",
        "data_test = {product: torch.tensor(get_final_series(product, data_reshaped)) for product in ['Corn', 'Soybeans', 'Wheat', 'Oats']}\n"
      ],
      "metadata": {
        "id": "cnpn1qqYWUS3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "vwxGjzf2WhC-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor3(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(Predictor3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(3, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size + 10, 8)\n",
        "        self.fc2 = nn.Linear(8, output_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_serial = x[:, :, -3:]\n",
        "        # print(x_serial)\n",
        "        x_static = x[:, 0, :-3]\n",
        "        # print(x_static)\n",
        "\n",
        "        out, (h_n, c_n) = self.lstm(x_serial)\n",
        "        # print(torch.cat([out[:, -1, :], x_static], dim=1))\n",
        "        out = F.relu(self.fc1(torch.cat([out[:, -1, :], x_static], dim=1)))\n",
        "        return self.fc2(out)"
      ],
      "metadata": {
        "id": "v-tzZ3MsX96d"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMPredictor2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(LSTMPredictor2, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(h_n)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rh2wj332WjZE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inputs(series, device):\n",
        "    x = series.unsqueeze(0).float().to(device)\n",
        "    x = x[:, :-1, :]\n",
        "    y = series[-1, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "1atJ0dXUuasz"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inputs2(series, device):\n",
        "    train_data_size = int(0.8 * series.shape[0])\n",
        "    train_data = series[:train_data_size]\n",
        "    test_data = series[train_data_size:-1]\n",
        "    train_x = train_data.unsqueeze(0).float().to(device)\n",
        "    val_x = test_data.unsqueeze(0).float().to(device)\n",
        "    train_y = series[train_data_size, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    val_y = series[-1, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    return train_x, train_y, val_x, val_y"
      ],
      "metadata": {
        "id": "USbBJscSO2tJ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Predictor3(13, 64, 2, 1, device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# A = torch.randn(1, 101, 13)\n",
        "# A[:, :, :-1] = 0\n",
        "# A = data_train[0].unsqueeze(0).float().to(device)\n",
        "running_loss = np.inf\n",
        "for epoch in range(1, 101):\n",
        "    previous_running_loss = running_loss\n",
        "    running_loss = 0\n",
        "    for data in data_train:\n",
        "        train_x, train_y = create_inputs(data, device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_x)\n",
        "\n",
        "        loss = criterion(outputs, train_y)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if abs(previous_running_loss - running_loss) < 1e-4:\n",
        "      print(f'Epoch {epoch+1}, Loss {running_loss / len(data_train)}')\n",
        "      break\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss {running_loss / len(data_train)}')\n",
        "    # optimizer.zero_grad()\n",
        "    # output = model(A[:, :-1, :])\n",
        "    # loss = criterion(output, A[:, 1:, -1].unsqueeze(2))\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "    # print(f'Epoch {epoch+1}, Loss {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IgWNJC0o0By",
        "outputId": "aea02151-cc61-48f3-d52e-966d055bf414"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss 0.6340332164196297\n",
            "Epoch 20, Loss 0.5243942360772053\n",
            "Epoch 30, Loss 0.45294591273086554\n",
            "Epoch 40, Loss 0.4111247294153145\n",
            "Epoch 50, Loss 0.3846084435066587\n",
            "Epoch 60, Loss 0.3663525033320184\n",
            "Epoch 70, Loss 0.352064409966988\n",
            "Epoch 80, Loss 0.3418859649362275\n",
            "Epoch 90, Loss 0.33388855189987227\n",
            "Epoch 100, Loss 0.32638242420507596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for product in data_test:\n",
        "        data = data_test[product]\n",
        "        val_x, val_y = create_inputs(data, device)\n",
        "        outputs = model(val_x)\n",
        "        # compute absolute error\n",
        "        error = (val_y - outputs) ** 2\n",
        "        try:\n",
        "            overall = torch.stack(overall, error)\n",
        "        except:\n",
        "            overall = error\n",
        "\n",
        "print('Test MSE:', overall.mean().item())"
      ],
      "metadata": {
        "id": "6vc6K7es8DyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for product in data_test:\n",
        "        data = data_test[product]\n",
        "        x = data.unsqueeze(0).float().to(device)\n",
        "        outputs = model(x)\n",
        "        print(f'Sharpe ratio prediction for {product} for next quarter: {outputs.item()}')"
      ],
      "metadata": {
        "id": "NmONiPfIBB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0Lul2loOLIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}