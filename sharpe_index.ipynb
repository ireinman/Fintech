{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHsYcXLbR5vN"
      },
      "source": [
        "# Sharpe Ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aPKj-iyVM2j"
      },
      "source": [
        "## Using LSTMs to Predict Sharpe Ratio of the Next Quarter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XCSCZ2bQwADG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf  # Yahoo Finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VlDxOTJGVI2H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3w4MiPSEkQ"
      },
      "source": [
        "In this notebook, we will be attempting to predict the quarterly sharpe ratio of several commodity futures. All data in this notebook was collected via Yahoo Finance's API. We will predict the sharpe ratio using the futures with the nearest expiration dates, specifically:\n",
        "\n",
        "1. Corn/Maize Futures (ZC=F), daily close values.\n",
        "2. Soybean Futures (ZS=F), daily close values.\n",
        "3. Wheat Futures (ZW=F), daily close values.\n",
        "4. Oats Futures (ZO=F), daily close values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQYW0U5SEkQ"
      },
      "source": [
        "To make predictions, we will use two different types of features - static and serial features. Static features are the word embedding (GloVe) of the name of the future contract (Corn / Soybeans / etc.), as they do not change with time there is no reason to use them in the LSTM model. On the other hand, serial features are the quarterly Sharpe Ratios and the years / quarters of the value. Overall, the data for each future contract includes a series of all sharpe ratios for every quarter available in yahoo finance and a vector of size 10 including the embeddings of the future contract's name and category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwoYTwc9SEkQ"
      },
      "source": [
        "The word embeddings come frome gensim, a python module that collected data from wikipedia to train its GloVe model. GloVe is an open-source project by Stanford launched in 2014. Briefly, GloVe learns a vector representation of words based on their neighborhoods and how they appear in each others contexts. The goal is for two words with embeddings $w_1, w_2$:\n",
        "\n",
        "$$\n",
        "w_i^T w_j \\approx \\ln P_{ij} + b_i + b_j\n",
        "$$\n",
        "\n",
        "where $P_{ij}$ is the probability that $j$ is in the context (some window around the word) of $i$ when $i$ is a randomly sampled occurence of $i$ in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqmVQj8tR5vT",
        "outputId": "d63f76c5-af04-44f7-b575-a5821c8ac78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load word vectors\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WsqgXPUXo91s"
      },
      "outputs": [],
      "source": [
        "def get_vector(string):\n",
        "    sol = np.zeros(100)\n",
        "    for word in string.lower().split():\n",
        "        try:\n",
        "            sol += word_vectors[word]\n",
        "        except:\n",
        "            continue\n",
        "    return sol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pPkS4Ud9m0no"
      },
      "outputs": [],
      "source": [
        "with open('commodities_tickers.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "categories = data.keys()\n",
        "categories_vectors = {category: get_vector(category) for category in categories}\n",
        "\n",
        "data_reshaped = {key2: [key1, value2] for key1, value1 in data.items() for key2, value2 in value1.items()}\n",
        "\n",
        "# make matrix of word vectors for each commodity\n",
        "commodities = data_reshaped.keys()\n",
        "commodities_vectors = np.array([get_vector(commodity) for commodity in commodities])\n",
        "\n",
        "# apply PCA to reduce the dimensionality of the word vectors\n",
        "pca = PCA(n_components=5)\n",
        "commodities_vectors_pca = pca.fit_transform(commodities_vectors)\n",
        "categories_vectors_pca = pca.transform(np.array(list(categories_vectors.values())))\n",
        "categories_vectors_pca = {key: value for key, value in zip(categories, categories_vectors_pca)}\n",
        "\n",
        "data_reshaped = {key: [np.concatenate([categories_vectors_pca[value[0]], commodities_vectors_pca[i]]), value[1]] for i, (key, value) in enumerate(data_reshaped.items())}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pomSwXoaSEkS"
      },
      "source": [
        "In order to compute the sharpe ratio, we must find a risk-free stock to compare our stocks' returns to. In this case we chose for the risk-free stock to be ^IRX (13 WEEK USA TREASURY BILL). All values used will be CLOSE values. The data collected is the maximum available in yahoo finance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c3UJcD9OP7cy"
      },
      "outputs": [],
      "source": [
        "RISK_FREE = yf.Ticker('^IRX')\n",
        "RISK_FREE = RISK_FREE.history(period='max')\n",
        "RISK_FREE['Datetime'] = RISK_FREE.index\n",
        "RISK_FREE.reset_index(drop=True, inplace=True)\n",
        "RISK_FREE['Date'] = RISK_FREE['Datetime'].dt.date\n",
        "RISK_FREE['Returns-RF'] = RISK_FREE['Close'].pct_change()\n",
        "\n",
        "def retrieve_feature(product, data_reshaped):\n",
        "    return yf.Ticker(data_reshaped[product][1]).history(period='max')\n",
        "\n",
        "def feature_extraction(data):\n",
        "    data['Datetime'] = data.index\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data['Date'] = data['Datetime'].dt.date\n",
        "    data.dropna(inplace=True)\n",
        "    data['Return'] = data['Close'].pct_change()\n",
        "    return data[['Datetime', 'Date', 'Return']]\n",
        "\n",
        "def risk_free_return(data):\n",
        "    data = data.merge(RISK_FREE[['Date', 'Returns-RF']], on='Date', how='left')\n",
        "    data.rename(columns={'Returns-RF': 'rf'}, inplace=True)\n",
        "    data.dropna(inplace=True)\n",
        "    data['Risk-Adjusted Return'] = data['Return'] - data['rf']\n",
        "    return data[['Datetime', 'Risk-Adjusted Return']]\n",
        "\n",
        "def sharpe_ratio(data):\n",
        "    # add feature of quarter such that jan/feb/mar -> 1, apr/may/jun -> 2, jul/aug/sep -> 3, oct/nov/dec -> 4\n",
        "    data['Quarter'] = data['Datetime'].dt.quarter\n",
        "    # group by quarter and year, compute the mean and standard deviation of risk-adjusted return\n",
        "    data_grouped = data.groupby([data['Datetime'].dt.year, 'Quarter'])\n",
        "\n",
        "    mean = data_grouped['Risk-Adjusted Return'].mean()\n",
        "    std = data_grouped['Risk-Adjusted Return'].std()\n",
        "    length = data_grouped['Risk-Adjusted Return'].count()\n",
        "\n",
        "    sharp_ratio = mean / std * (length ** 0.5)\n",
        "    return sharp_ratio.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gRQAYSZDP9ND"
      },
      "outputs": [],
      "source": [
        "def get_final_series(product, data_reshaped):\n",
        "    \"\"\"\n",
        "    Given the name of a product and the dictionary of all products, return the financial data of the product, the risk-adjusted return, and the Sharpe ratio.\n",
        "    \"\"\"\n",
        "    data = retrieve_feature(product, data_reshaped)\n",
        "    data = feature_extraction(data)\n",
        "    data = risk_free_return(data)\n",
        "    sharpe = sharpe_ratio(data)\n",
        "    values = sharpe.values\n",
        "    # for every row in values, concatenate the corresponding vector in data_reshaped\n",
        "    meta_data = data_reshaped[product][0]\n",
        "    # copy meta_data for each row in values\n",
        "    meta_data = np.tile(meta_data, (values.shape[0], 1))\n",
        "    return np.concatenate([meta_data, values], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyky6NwUSEkS"
      },
      "source": [
        "We will train on all financial data that we have except for the cereals we are interested in predicting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cnpn1qqYWUS3"
      },
      "outputs": [],
      "source": [
        "data_train = [torch.tensor(get_final_series(product, data_reshaped)) for product in data_reshaped.keys() if product not in ['Corn', 'Soybeans', 'Wheat', 'Oats']]\n",
        "data_test = {product: torch.tensor(get_final_series(product, data_reshaped)) for product in ['Corn', 'Soybeans', 'Wheat', 'Oats']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOiaXdnKSEkT"
      },
      "source": [
        "Predictor is an LSTM. The theoretical background of LSTMs is in the file deep_sequential.ipynb. Hyperparameter training was used, but we only present the final product.\n",
        "\n",
        "Only the sequential data is inserted to the LSTM part, because the static data is the same over all time points. After the LSTM returns a final vector for the serial input of values (year/quarter/sharpe ratio), we concatenate that information with the meta-information about the futures (GloVe embeddings) and insert it into a fully connected network with one hidden layer and a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v-tzZ3MsX96d"
      },
      "outputs": [],
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(3, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size + 10, 8)\n",
        "        self.fc2 = nn.Linear(8, output_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_serial = x[:, :, -3:]\n",
        "        # print(x_serial)\n",
        "        x_static = x[:, 0, :-3]\n",
        "        # print(x_static)\n",
        "\n",
        "        out, (h_n, c_n) = self.lstm(x_serial)\n",
        "        # print(torch.cat([out[:, -1, :], x_static], dim=1))\n",
        "        out = F.relu(self.fc1(torch.cat([out[:, -1, :], x_static], dim=1)))\n",
        "        return self.fc2(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1atJ0dXUuasz"
      },
      "outputs": [],
      "source": [
        "def create_inputs(series, device):\n",
        "    \"\"\"\n",
        "    Every vector in the sequence should predict the last index (sharpe ratio) of the next vector in the sequence. Hence the last row of the input should not be used. In the end, we are only interested in predicting the final sharpe ratio of the sequence.\n",
        "    \"\"\"\n",
        "    x = series.unsqueeze(0).float().to(device)\n",
        "    x = x[:, :-1, :]\n",
        "    y = series[-1, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "USbBJscSO2tJ"
      },
      "outputs": [],
      "source": [
        "def create_inputs2(series, device):\n",
        "    train_data_size = int(0.8 * series.shape[0])\n",
        "    train_data = series[:train_data_size]\n",
        "    test_data = series[train_data_size:-1]\n",
        "    train_x = train_data.unsqueeze(0).float().to(device)\n",
        "    val_x = test_data.unsqueeze(0).float().to(device)\n",
        "    train_y = series[train_data_size, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    val_y = series[-1, -1].unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    return train_x, train_y, val_x, val_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IgWNJC0o0By",
        "outputId": "d2d8243e-de15-4654-9f6b-912a1800a148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 0.9990135297801317\n",
            "Epoch 10, Loss 0.49878592299646696\n",
            "Epoch 20, Loss 0.4208293967647478\n",
            "Epoch 30, Loss 0.3849711169543298\n",
            "Epoch 40, Loss 0.3612002514518281\n",
            "Epoch 50, Loss 0.3445150070904447\n",
            "Epoch 60, Loss 0.32934238641610136\n",
            "Epoch 70, Loss 0.3140702413373447\n",
            "Epoch 80, Loss 0.30151009191991995\n",
            "Epoch 90, Loss 0.2901322982099373\n",
            "Epoch 100, Loss 0.27983164646536807\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Predictor(13, 64, 2, 1, device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "running_loss = np.inf\n",
        "for epoch in range(1, 101):\n",
        "    previous_running_loss = running_loss\n",
        "    running_loss = 0\n",
        "    for data in data_train:\n",
        "        train_x, train_y = create_inputs(data, device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_x)\n",
        "\n",
        "        loss = criterion(outputs, train_y)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if abs(previous_running_loss - running_loss) < 1e-4:\n",
        "      print(f'Epoch {epoch}, Loss {running_loss / len(data_train)}')\n",
        "      break\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f'Epoch {epoch}, Loss {running_loss / len(data_train)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6vc6K7es8DyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0e5858-f9cb-4aad-cea5-20beab298bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 4.091559410095215\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for product in data_test:\n",
        "        data = data_test[product]\n",
        "        val_x, val_y = create_inputs(data, device)\n",
        "        outputs = model(val_x)\n",
        "        # compute absolute error\n",
        "        error = (val_y - outputs) ** 2\n",
        "        try:\n",
        "            overall = torch.stack(overall, error)\n",
        "        except:\n",
        "            overall = error\n",
        "\n",
        "print('Test MSE:', overall.mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately we get a relatively high MSE on our test set. This score is high because Sharpe ratio doesn't tend to reach such high values as it is normalized. This may be because our model overfit the training data, or many other reasons such as not taking macro events into consideration, Sharpe ratio containing a large element of randomness, etc. Regardless, below we can see the predictions of the Sharpe Ratios in the next quarter (final quarter of 2024), based on all past data and the embeddings."
      ],
      "metadata": {
        "id": "9Ln0zb_dSWCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NmONiPfIBB1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a9a915-021c-477f-832f-311c59a9cc0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sharpe ratio prediction for Corn for next quarter: -0.43392103910446167\n",
            "Sharpe ratio prediction for Soybeans for next quarter: -0.33158957958221436\n",
            "Sharpe ratio prediction for Wheat for next quarter: -0.4126572608947754\n",
            "Sharpe ratio prediction for Oats for next quarter: -0.15434390306472778\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for product in data_test:\n",
        "        data = data_test[product]\n",
        "        x = data.unsqueeze(0).float().to(device)\n",
        "        outputs = model(x)\n",
        "        print(f'Sharpe ratio prediction for {product} for next quarter: {outputs.item()}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}